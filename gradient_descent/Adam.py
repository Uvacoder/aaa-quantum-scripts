import matplotlib.pyplot as plt
import numpy as np


def f(x, w, b):
    # Sigmoid Function
    f = 1 / (1 + np.exp(-(w * x + b)))
    return f


def mse(x, y, w, b):
    # Mean Squared Loss Function
    L = 0.0
    for i in range(x.shape[0]):
        L += 0.5 * (y[i] - f(x[i], w, b)) ** 2
    return L


def cross_entropy(x, y, w, b):
    # Cross Entropy Loss Function
    L = 0.0
    for i in range(x.shape[0]):
        L += -(y[i] * np.log(f(x[i], w, b)))
    return L


def grad_w_mse(x, y, w, b):
    fx = f(x, w, b)
    dw = (fx - y) * fx * (1 - fx) * x
    return dw


def grad_b_mse(x, y, w, b):
    fx = f(x, w, b)
    db = (fx - y) * fx * (1 - fx)
    return db


def grad_w_cross(x, y, w, b):
    fx = f(x, w, b)
    dw = (-y) * (1 - fx) * x
    return dw


def grad_b_cross(x, y, w, b):
    fx = f(x, w, b)
    db = (-y) * (1 - fx)
    return db


def Adam(x, y, epochs, batch_size, loss, lr):
    w = np.random.randn()
    b = np.random.randn()
    epsilon = 1e-8
    beta1 = 0.9
    beta2 = 0.999
    momentum_w, momentum_b = 0, 0
    update_w, update_b = 0, 0
    l_list = []
    w_list = []
    b_list = []
    points = 0
    ep = [i for i in range(epochs + 1)]
    dw, db = 0, 0
    for i in range(epochs + 1):
        dw, db = 0, 0
        for j in range(x.shape[0]):
            if loss == "mse":
                dw += grad_w_mse(x[j], y[j], w, b)
                db += grad_b_mse(x[j], y[j], w, b)
            elif loss == "cross_entropy":
                dw += grad_w_cross(x[j], y[j], w, b)
                db += grad_b_cross(x[j], y[j], w, b)
            points += 1
            if points % batch_size == 0:
                # Momentum
                momentum_w = beta1 * momentum_w + (1 - beta1) * dw
                momentum_b = beta1 * momentum_b + (1 - beta1) * db
                # Update History
                update_w = beta2 * update_w + (1 - beta2) * dw ** 2
                update_b = beta2 * update_b + (1 - beta2) * db ** 2
                # Bias Correction
                momentum_w = momentum_w / (1 - math.pow(beta1, i + 1))
                momentum_b = momentum_b / (1 - math.pow(beta1, i + 1))
                update_w = update_w / (1 - math.pow(beta2, i + 1))
                update_b = update_b / (1 - math.pow(beta2, i + 1))
                # Update of Parameters
                w = w - (lr / np.sqrt(update_w + epsilon)) * momentum_w
                b = b - (lr / np.sqrt(update_b + epsilon)) * momentum_b
                dw, db = 0, 0
        if loss == "mse":
            print("Loss after {}th epoch = {}\n".format(i, mse(x, y, w, b)[0]))
            l_list.append(mse(x, y, w, b)[0])
        elif loss == "cross_entropy":
            print(
                "Loss after {}th epoch = {}\n".format(i, cross_entropy(x, y, w, b)[0])
            )
            l_list.append(cross_entropy(x, y, w, b)[0])
        w_list.append(w[0])
        b_list.append(b[0])
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title(
        "Loss vs Epoch Curve\nAlgotithm :Mini Batch Adam\nBatch Size = {}\nInitial Learning Rate = {}\nLoss Function = {}".format(
            batch_size, lr, loss
        )
    )
    plt.plot(ep, l_list)
    plt.show()
    return w_list, b_list
